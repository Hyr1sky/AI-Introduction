{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b1de364",
   "metadata": {},
   "source": [
    "# 5.3 PyTorch神经网络工具箱\n",
    "\n",
    "PyTorch中的nn模块是专门为神经网络设计的模块化接口工具箱。\n",
    "\n",
    "## 5.3.1 一维卷积类nn.Conv1d\n",
    "\n",
    "一维卷积常用于文本等序列数据，只对宽度进行卷积，对高度不卷积。例如，含有L各词（或字）的文本，可认为其长度为L，每个词或字的特征（或称为嵌入向量）大小为D，则文本的特征可表示为形状为(D，L)的张量，卷积核窗口在句子长度的方向上滑动，即可进行一维卷积操作。\n",
    "\n",
    "下面给出一维卷积的示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef86af36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100, 33])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "conv1 = nn.Conv1d(in_channels=256, out_channels=100, kernel_size=3, stride=1, padding=0)  # 定义一个一维卷积实例\n",
    "input = torch.randn(32, 35, 256)  # 定义输入特征图张量，形状为[batch_size, L, D],参数分别为批大小batch_size、最大长度L和特征维度D\n",
    "input = input.permute(0, 2, 1)  # 张量交换维度，形状变为[batch_size, D, L]\n",
    "out = conv1(input)  #进行一维卷积操作，输出特征图张量形状为[batch_size, out_channels, (L + 2 * padding - kernel_size) / stride + 1]\n",
    "print(out.shape)  # 打印输出张量的形状"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dea9070",
   "metadata": {},
   "source": [
    "分析以上代码已知，input张量的形状是[batch_size, L, D]，在开始Conv1d前需要将L换到最后一个维度，卷积核在该维度上滑动进行一维卷积。卷积后的结果的形状为[batch_size, out_channels, (L + 2 * padding - kernel_size) / stride + 1]，在纵向维度上的值为(35 + 2*0 - 3)/1 + 1=33，即卷积后序列的特征图长度由原来的35变为了33。\n",
    "\n",
    "一维卷积中含有要学习的参数，其中权值参数个数为in_channels * out_channels * kernel_size，若带有偏置项，则偏置项个数为out_channels。上例中，可以用如下代码打印所有参数的当前值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5902cb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[[-0.0217, -0.0190, -0.0277],\n",
      "         [-0.0032, -0.0023, -0.0356],\n",
      "         [-0.0073,  0.0023, -0.0221],\n",
      "         ...,\n",
      "         [ 0.0064, -0.0235, -0.0195],\n",
      "         [-0.0043, -0.0019, -0.0160],\n",
      "         [ 0.0285,  0.0345,  0.0334]],\n",
      "\n",
      "        [[ 0.0205,  0.0354, -0.0247],\n",
      "         [ 0.0121,  0.0197, -0.0333],\n",
      "         [ 0.0223, -0.0299,  0.0344],\n",
      "         ...,\n",
      "         [ 0.0075, -0.0298,  0.0130],\n",
      "         [ 0.0001,  0.0258, -0.0264],\n",
      "         [-0.0227,  0.0062, -0.0156]],\n",
      "\n",
      "        [[-0.0059, -0.0349,  0.0256],\n",
      "         [-0.0280, -0.0288, -0.0291],\n",
      "         [-0.0194, -0.0004,  0.0214],\n",
      "         ...,\n",
      "         [-0.0278,  0.0264,  0.0353],\n",
      "         [ 0.0285, -0.0163, -0.0147],\n",
      "         [ 0.0028,  0.0240, -0.0045]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0305, -0.0228, -0.0079],\n",
      "         [ 0.0208, -0.0116, -0.0079],\n",
      "         [-0.0034, -0.0167, -0.0026],\n",
      "         ...,\n",
      "         [ 0.0253, -0.0160,  0.0176],\n",
      "         [ 0.0074, -0.0003,  0.0332],\n",
      "         [ 0.0270,  0.0352,  0.0115]],\n",
      "\n",
      "        [[ 0.0047, -0.0301,  0.0057],\n",
      "         [ 0.0010, -0.0143,  0.0239],\n",
      "         [-0.0349,  0.0249, -0.0152],\n",
      "         ...,\n",
      "         [-0.0024, -0.0047,  0.0315],\n",
      "         [-0.0271,  0.0156, -0.0124],\n",
      "         [ 0.0051, -0.0235, -0.0262]],\n",
      "\n",
      "        [[-0.0330, -0.0152,  0.0247],\n",
      "         [-0.0223,  0.0209, -0.0094],\n",
      "         [-0.0312, -0.0064,  0.0170],\n",
      "         ...,\n",
      "         [-0.0025, -0.0012,  0.0106],\n",
      "         [ 0.0026, -0.0135,  0.0147],\n",
      "         [ 0.0287,  0.0115,  0.0200]]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0031,  0.0084, -0.0326,  0.0278, -0.0008,  0.0242, -0.0291,  0.0176,\n",
      "        -0.0343, -0.0105, -0.0201, -0.0296, -0.0267, -0.0095, -0.0310, -0.0237,\n",
      "         0.0356, -0.0345, -0.0082, -0.0169, -0.0064, -0.0019,  0.0066,  0.0218,\n",
      "        -0.0318,  0.0087, -0.0220,  0.0094,  0.0204,  0.0083,  0.0181,  0.0024,\n",
      "        -0.0206, -0.0299,  0.0226,  0.0307,  0.0032, -0.0247,  0.0246,  0.0047,\n",
      "        -0.0346, -0.0312,  0.0177, -0.0015,  0.0011,  0.0073,  0.0070, -0.0329,\n",
      "        -0.0026,  0.0206, -0.0355, -0.0324, -0.0087,  0.0119, -0.0326, -0.0107,\n",
      "        -0.0108,  0.0111, -0.0154, -0.0016,  0.0340,  0.0035,  0.0121,  0.0105,\n",
      "         0.0036, -0.0165,  0.0052, -0.0131,  0.0166,  0.0337,  0.0282,  0.0249,\n",
      "        -0.0106, -0.0124, -0.0135,  0.0178,  0.0002, -0.0060, -0.0092, -0.0330,\n",
      "        -0.0266,  0.0206, -0.0142, -0.0274,  0.0281,  0.0239, -0.0297, -0.0230,\n",
      "         0.0002,  0.0019,  0.0353,  0.0070, -0.0008,  0.0230, -0.0095, -0.0247,\n",
      "        -0.0304, -0.0120,  0.0086, -0.0210], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(conv1.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5c5349",
   "metadata": {},
   "source": [
    "## 5.3.2 二维卷积类nn.Conv2d\n",
    "\n",
    "二维卷积常用于图像数据，同时对宽和高进行卷积处理。对于输入为(channel, height, width)的图像，其中channel为图像的通道数，height为图像的高度，width为图像的宽度，卷积核从左到右，从上到下对图像进行卷积操作。\n",
    "\n",
    "假设输入张量数据形状为[10, 16, 64, 64]，表示batch_size为10，即一次输入10张二维矩阵数据，每组数据的通道数为16，矩阵的宽度和高度分别为64和64。设卷积核大小为(16,3,3)，则进行二维卷积的示例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4267cb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 62, 62])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10, 16, 64, 64)  # 参数分别为batch_size, channel, height, width\n",
    "m = nn.Conv2d(16, 32, (3, 3), (1, 1))  # in_channel, out_channel ,kennel_size,stride\n",
    "y = m(x)\n",
    "print(y.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a03d64",
   "metadata": {},
   "source": [
    "二维卷积对宽和高的卷积结果形状都为[batch_size, out_channels, (L + 2 * padding - kernel_size) / stride + 1]，其中L为矩阵数据的宽或高，stride都为1，padding默认为0，所以输出张量横向和纵向的维度均为(64+2*0-3)/1 + 1 = 62。\n",
    "\n",
    "二维卷积中也含有要学习的参数，其中权值参数个数为in_channels * out_channels * kernel_size * kernel_size，若带有偏置项，则偏置项个数为out_channels。和观察一维卷积权值参数类似，上例中也可以用如下代码打印所有参数的当前值：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb49aa48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[[[ 0.0801, -0.0273, -0.0643],\n",
      "          [ 0.0789,  0.0441, -0.0523],\n",
      "          [ 0.0166,  0.0822, -0.0010]],\n",
      "\n",
      "         [[ 0.0655,  0.0538,  0.0600],\n",
      "          [ 0.0034,  0.0211, -0.0735],\n",
      "          [-0.0267,  0.0601, -0.0425]],\n",
      "\n",
      "         [[ 0.0259, -0.0742, -0.0286],\n",
      "          [ 0.0156,  0.0514,  0.0310],\n",
      "          [-0.0250, -0.0490,  0.0581]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0803,  0.0046,  0.0509],\n",
      "          [ 0.0830, -0.0300, -0.0612],\n",
      "          [ 0.0380,  0.0217,  0.0215]],\n",
      "\n",
      "         [[-0.0197, -0.0788,  0.0693],\n",
      "          [ 0.0647,  0.0369, -0.0811],\n",
      "          [ 0.0130, -0.0064,  0.0568]],\n",
      "\n",
      "         [[-0.0365, -0.0166,  0.0423],\n",
      "          [-0.0661, -0.0087,  0.0152],\n",
      "          [ 0.0707, -0.0078, -0.0584]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0002,  0.0160, -0.0259],\n",
      "          [-0.0423, -0.0599,  0.0428],\n",
      "          [-0.0657,  0.0787,  0.0359]],\n",
      "\n",
      "         [[ 0.0262,  0.0137,  0.0631],\n",
      "          [-0.0128, -0.0151, -0.0154],\n",
      "          [ 0.0109, -0.0255, -0.0722]],\n",
      "\n",
      "         [[ 0.0391,  0.0035, -0.0404],\n",
      "          [ 0.0301,  0.0679,  0.0609],\n",
      "          [ 0.0830, -0.0256,  0.0442]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0546, -0.0533, -0.0618],\n",
      "          [-0.0401, -0.0477,  0.0399],\n",
      "          [ 0.0591,  0.0603, -0.0560]],\n",
      "\n",
      "         [[-0.0303, -0.0623,  0.0718],\n",
      "          [-0.0480,  0.0746,  0.0093],\n",
      "          [ 0.0072,  0.0222,  0.0511]],\n",
      "\n",
      "         [[ 0.0803,  0.0240, -0.0252],\n",
      "          [ 0.0754,  0.0019, -0.0369],\n",
      "          [-0.0741, -0.0489, -0.0207]]],\n",
      "\n",
      "\n",
      "        [[[-0.0411, -0.0229, -0.0285],\n",
      "          [-0.0098,  0.0478, -0.0195],\n",
      "          [-0.0640,  0.0329, -0.0795]],\n",
      "\n",
      "         [[-0.0249,  0.0414,  0.0399],\n",
      "          [-0.0734, -0.0284,  0.0429],\n",
      "          [-0.0195,  0.0214, -0.0746]],\n",
      "\n",
      "         [[ 0.0598, -0.0412,  0.0458],\n",
      "          [-0.0793, -0.0645, -0.0625],\n",
      "          [-0.0704, -0.0573, -0.0075]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0442,  0.0413, -0.0038],\n",
      "          [ 0.0275,  0.0061, -0.0224],\n",
      "          [-0.0565, -0.0273,  0.0289]],\n",
      "\n",
      "         [[-0.0709,  0.0646,  0.0353],\n",
      "          [-0.0478, -0.0342,  0.0498],\n",
      "          [-0.0475, -0.0146,  0.0538]],\n",
      "\n",
      "         [[ 0.0479, -0.0731,  0.0478],\n",
      "          [ 0.0194, -0.0575,  0.0022],\n",
      "          [-0.0026, -0.0319, -0.0581]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0156, -0.0013,  0.0633],\n",
      "          [ 0.0779,  0.0477,  0.0252],\n",
      "          [-0.0264,  0.0566, -0.0275]],\n",
      "\n",
      "         [[ 0.0226, -0.0831,  0.0816],\n",
      "          [ 0.0769,  0.0589,  0.0817],\n",
      "          [-0.0115, -0.0040, -0.0025]],\n",
      "\n",
      "         [[-0.0147, -0.0686,  0.0416],\n",
      "          [-0.0523,  0.0618,  0.0035],\n",
      "          [-0.0516, -0.0379,  0.0326]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0601,  0.0734,  0.0498],\n",
      "          [ 0.0706, -0.0143, -0.0459],\n",
      "          [-0.0181, -0.0609,  0.0338]],\n",
      "\n",
      "         [[-0.0362,  0.0002, -0.0071],\n",
      "          [ 0.0125,  0.0616, -0.0746],\n",
      "          [-0.0461,  0.0189,  0.0008]],\n",
      "\n",
      "         [[-0.0829,  0.0560, -0.0754],\n",
      "          [ 0.0594, -0.0222, -0.0090],\n",
      "          [-0.0186, -0.0102, -0.0760]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0725, -0.0229,  0.0590],\n",
      "          [ 0.0350, -0.0434,  0.0425],\n",
      "          [ 0.0715, -0.0419,  0.0234]],\n",
      "\n",
      "         [[ 0.0629, -0.0721,  0.0520],\n",
      "          [ 0.0142,  0.0255,  0.0592],\n",
      "          [ 0.0017,  0.0495, -0.0824]],\n",
      "\n",
      "         [[-0.0191,  0.0555, -0.0357],\n",
      "          [ 0.0125,  0.0540,  0.0829],\n",
      "          [ 0.0747,  0.0593, -0.0418]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0373, -0.0095,  0.0319],\n",
      "          [-0.0683, -0.0697, -0.0166],\n",
      "          [-0.0177, -0.0042, -0.0799]],\n",
      "\n",
      "         [[-0.0695, -0.0367, -0.0197],\n",
      "          [ 0.0598,  0.0325,  0.0779],\n",
      "          [-0.0163,  0.0708,  0.0555]],\n",
      "\n",
      "         [[-0.0343,  0.0061, -0.0067],\n",
      "          [-0.0344,  0.0143,  0.0196],\n",
      "          [-0.0606, -0.0255, -0.0297]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0279, -0.0777, -0.0114],\n",
      "          [ 0.0403,  0.0829, -0.0825],\n",
      "          [-0.0242,  0.0649, -0.0029]],\n",
      "\n",
      "         [[-0.0091,  0.0169, -0.0131],\n",
      "          [-0.0413, -0.0405, -0.0270],\n",
      "          [-0.0444, -0.0831, -0.0471]],\n",
      "\n",
      "         [[ 0.0557, -0.0208, -0.0173],\n",
      "          [ 0.0222,  0.0017,  0.0537],\n",
      "          [ 0.0538, -0.0754,  0.0775]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0270,  0.0375, -0.0149],\n",
      "          [-0.0089, -0.0146,  0.0638],\n",
      "          [-0.0778,  0.0256,  0.0232]],\n",
      "\n",
      "         [[ 0.0245, -0.0314,  0.0707],\n",
      "          [-0.0473, -0.0420,  0.0225],\n",
      "          [ 0.0557, -0.0759,  0.0770]],\n",
      "\n",
      "         [[ 0.0800,  0.0614,  0.0115],\n",
      "          [ 0.0547,  0.0148,  0.0542],\n",
      "          [ 0.0564,  0.0099,  0.0718]]]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0440, -0.0764,  0.0250, -0.0631,  0.0243,  0.0785, -0.0116,  0.0382,\n",
      "         0.0139,  0.0229,  0.0689,  0.0396,  0.0350,  0.0250,  0.0438,  0.0171,\n",
      "        -0.0096, -0.0561, -0.0428,  0.0812,  0.0116,  0.0038,  0.0628,  0.0272,\n",
      "         0.0234,  0.0343,  0.0247, -0.0575, -0.0670, -0.0818,  0.0443,  0.0768],\n",
      "       requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(m.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712e3532",
   "metadata": {},
   "source": [
    "## 5.3.3 全连接类nn.Linear\n",
    "\n",
    "全连接类作用于设置网络中的全连接层，对输入数据进行线性转换，并存储权重和偏置。通常，全连接操作的输入与输出都是二维张量，一般形状为[batch_size, size]，不同于二维卷积要求输入和输出的是四维张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8735b9",
   "metadata": {},
   "source": [
    "参数in_features表示输入维度的大小，out_features表示输出维度的大小，bias表示是否带偏置，默认是带偏置的。示例代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1a131e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "connected_layer = nn.Linear(in_features = 64*64*3, out_features = 1)  # 输入维度为64*64*3，输出维度为1\n",
    "input = torch.randn(10, 3, 64, 64)\n",
    "input = input.view(10, 64*64*3)  #torch.Size([10, 12288])\n",
    "output = connected_layer(input) # 调用全连接层\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c17eac",
   "metadata": {},
   "source": [
    "上例中，input调用view函数完成了对张量形状的调整，由原来的(10, 3, 64, 64)调整为(10, 12288)。在这里，也可以使用reshape()方法对张量形状进行调整，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a20ad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = input.reshape((10, 64*64*3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46326c55",
   "metadata": {},
   "source": [
    "全连接层中也含有要学习的参数，其中权值参数个数为in_features * out_features，若带有偏置项，则偏置项个数为out_features。和前面类似，上例中也可以用如下代码打印所有参数的当前值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ed832a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.0057,  0.0075,  0.0081,  ..., -0.0037, -0.0030,  0.0077]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0.0076], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(connected_layer.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e23d87f",
   "metadata": {},
   "source": [
    "## 5.3.4 平坦化类nn.Flatten\n",
    "\n",
    "上一节的示例中，input = input.view(10, 64*64*3)语句实现了对张量形状的调整。实际上，nn模块还提供了Flatten类，也可以直接把指定的连续几维数据展平为连续的一维数据，默认从第1维到最后一维进行平坦化，第0维常表示batch_size，因此不进行展平。\n",
    "\n",
    "由此，上例中的input = input.view(10, 64*64*3)语句，也可以直接写成："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a34ff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flatten = nn.Flatten()  # 实例化类对象\n",
    "input = Flatten(input)  # 进行展平，input的shape也是torch.Size([10, 12288])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008f7f43",
   "metadata": {},
   "source": [
    "可见，两者的效果是一样的。但nn.Flatten作为一种操作，可以放到顺序化容器（nn.Sequential）中，更具有通用性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c3040a",
   "metadata": {},
   "source": [
    "## 5.3.5 非线性激活函数\n",
    "\n",
    "PyTorch的nn模块提供了丰富的非线性激活函数，用来对模型的输入和输出构建复杂的映射。例如ReLU、Softmax、Sigmoid、Tanh、LogSigmoid、LogSoftmax等。激活函数常用于在线性变换后，通过加入非线性变换使得模型能进行更复杂的表示。\n",
    "\n",
    "下面以nn.ReLU为例，介绍激活函数的使用。ReLU类定义如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd438375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReLU()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.ReLU(inplace = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b7fdc5",
   "metadata": {},
   "source": [
    "下面的代码是在线性层后引入非线性层的示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c67a0067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5])\n",
      "Before ReLU: tensor([[ 0.4898, -0.8552,  0.3207, -0.6352,  1.1304],\n",
      "        [ 0.0442,  0.4436,  0.1707,  0.6401,  0.2983],\n",
      "        [ 0.3294,  1.2857, -0.2988, -0.1367, -1.0043],\n",
      "        [ 0.5604, -0.0085,  0.1123, -1.1393,  0.4354]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.4898, 0.0000, 0.3207, 0.0000, 1.1304],\n",
      "        [0.0442, 0.4436, 0.1707, 0.6401, 0.2983],\n",
      "        [0.3294, 1.2857, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5604, 0.0000, 0.1123, 0.0000, 0.4354]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(4, 3, 64, 64)\n",
    "Flatten = nn.Flatten()  # 实例化类对象\n",
    "flat_image = Flatten(input)  # 进行展平，input的shape也是torch.Size([10, 12288])\n",
    "layer1 = nn.Linear(in_features=64*64*3, out_features=5)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())\n",
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80d9838",
   "metadata": {},
   "source": [
    "不难发现，经过ReLU函数后，所有的负值都变为了0。其他激活函数使用方法类似，将输入数据进行非线性处理。通常，激活函数都没有要学习的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369a992a",
   "metadata": {},
   "source": [
    "## 5.3.6 顺序化容器nn.Sequential\n",
    "\n",
    "顺序化容器可以将神经网络的各个模块，如卷积操作、全连接操作、激活函数等，按照顺序加入其中，模型在训练或推理时，按顺序执行各个操作。除此之外，也可以将多个模块放在有序字典里面进行传递。\n",
    "下面给出几种顺序化容器的实例化方法。\n",
    "\n",
    "（1）定义时直接加入模块\n",
    "\n",
    "示例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffd29363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tensor([[-0.0147,  0.1290],\n",
      "        [ 0.0379,  0.0206],\n",
      "        [ 0.0873,  0.2238],\n",
      "        [ 0.1602,  0.0237]], grad_fn=<AddmmBackward0>)\n",
      "Sequential(\n",
      "  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Flatten(start_dim=1, end_dim=-1)\n",
      "  (3): Linear(in_features=123008, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(4, 3, 64, 64)\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(3, 32, (3, 3), (1, 1)),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(32*62*62, 2),\n",
    ")\n",
    "output = net(input)\n",
    "print(f\"output: {output}\")  # 打印输出\n",
    "print(net)  # 打印网络模型结构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4082474d",
   "metadata": {},
   "source": [
    "（2）先定义对象，后加入模块\n",
    "\n",
    "示例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fd31599",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add_module('conv1', nn.Conv2d(16, 32, (3, 3), (1, 1)))  # 该卷积层命名为conv1\n",
    "net.add_module('relu', nn.ReLU())  # 该层命名为relu\n",
    "net.add_module('flatten', nn.Flatten())  # 该层命名为flatten\n",
    "net.add_module('linear', nn.Linear(32*62*62, 1))  # 该全连接层命名为linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f421c51d",
   "metadata": {},
   "source": [
    "该方法使用add_module函数将模块加入到计算图中，每层都有命名。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41756192",
   "metadata": {},
   "source": [
    "（3）定义时传入有序字典作为参数\n",
    "\n",
    "示例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62faa150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "net = nn.Sequential(OrderedDict([\n",
    "('conv1', nn.Conv2d(16, 32, (3, 3), (1, 1))),\n",
    "('relu', nn.ReLU()),\n",
    "('flatten', nn.Flatten()),\n",
    "          ('linear', nn.Linear(32*62*62, 1)),\n",
    "         ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d8ecaf",
   "metadata": {},
   "source": [
    "该方法将有序字典作为参数传入，各个神经网络模块作为有序字典的元素。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7d56dc",
   "metadata": {},
   "source": [
    "## 5.3.7 损失函数\n",
    "\n",
    "PyTorch提供了目前常用的各种损失函数的实现，下面列举一些常用的损失函数示例。\n",
    "\n",
    "（1）nn.L1Loss\n",
    "\n",
    "L1Loss即L1损失，也称为平均绝对误差损失（MAE，Mean Absolute Error），其定义如下：\n",
    "\n",
    "$$ Loss(y)=\\frac{1}{m} ∑_{i=1}^{m}|y_{i}-y ̂_{i} |$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cd446e",
   "metadata": {},
   "source": [
    "下面给出计算L1Loss的示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b45d876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2751, grad_fn=<L1LossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "loss = nn.L1Loss()\n",
    "predict_value = torch.randn(1, 23, requires_grad=True)\n",
    "target = torch.randn(1, 23)\n",
    "output = loss(predict_value, target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097f7dcf",
   "metadata": {},
   "source": [
    "(2）nn.MSELoss\n",
    "\n",
    " MSELoss即平均平方误差（或称均方误差）损失函数，是计算两组数据之间的均方误差，公式如下：\n",
    "\n",
    "$$ Loss(y)=\\frac{1}{m} ∑_{i=1}^{m}(y_{i}-y ̂_{i})^2 $$\n",
    "\n",
    "nn.MSELoss的使用方法与nn.L1Loss类似，在此不再给出示例。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20df19cb",
   "metadata": {},
   "source": [
    "（3）nn.CrossEntropyLoss\n",
    "\n",
    "CrossEntropyLoss是进行分类时常用的交叉熵损失函数，可以捕捉不同模型预测效果的差异。对于属于C个类的m个样本，设每个样本i的针对每个类c标签为yic，对于正确标签值为1，错误标签值为0，观测到样本i属于类别c的预测概率为pic，则交叉熵损失函数公式如下：\n",
    "\n",
    "$$ Loss(y)=\\frac{1}{m} ∑_{i=1}^{m}∑_{c=1}^{c}y_{ic}log(p_{ic})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91545e60",
   "metadata": {},
   "source": [
    "下面给出计算CrossEntropyLoss的示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95ca7705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p:tensor([[ 1.5410, -0.2934, -2.1788,  0.5684, -1.0845],\n",
      "        [-1.3986,  0.4033,  0.8380, -0.7193, -0.4033],\n",
      "        [-0.5966,  0.1820, -0.8567,  1.1006, -1.0712]], requires_grad=True)\n",
      "y:tensor([3, 0, 0])\n",
      "loss:2.272947072982788\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "p = torch.randn(3, 5, requires_grad=True)  # 得到5个输出结点的值(batch_size=3)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)  # 得到每个样本的实际类别标签\n",
    "print(f'p:{p}\\ny:{target}')\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "output = loss(p, target)  # 计算交叉熵损失\n",
    "print(f'loss:{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd72fbd",
   "metadata": {},
   "source": [
    "示例中可以看出，损失函数的第一个输入参数就是前一层的输出，有时称为Logits输出，可能存在负值或大于1的值，均不能代表概率，第二个输入参数是各个样本的标签数据。损失函数在内部计算时，为了得到样本属于各个类别的概率，通常需要经过softmax函数（或sigmoid函数），然后再可以进行交叉熵损失的计算。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
