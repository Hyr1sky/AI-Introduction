{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62cffb96",
   "metadata": {},
   "source": [
    "# 5.4 PyTorch常用工具介绍\n",
    "\n",
    "## 5.4.1 优化器\n",
    "\n",
    "优化器（optimzier）是指根据神经网络反向传播的梯度信息来自动更新网络模型的参数，以起到降低loss函数计算值的作用。PyTorch将深度学习中的常用参数更新优化方法全部封装在torch.optim包中。\n",
    "\n",
    "通常，我们直接使用优化器完成自动参数更新，使模型更好的收敛，获得更好的训练效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "810bedb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(model.parameters(), lr = lr)  # SGD优化器实例化\n",
    "# momentum动量加速，在SGD函数里指定momentum的值即可\n",
    "opt_Momentum = optim.SGD(model.parameters(), lr=lr, momentum=0.8)\n",
    "# RMSprop, 需要设置超参数alpha\n",
    "opt_RMSprop = optim.RMSprop(model.parameters(), lr=lr, alpha=0.9) \n",
    "# Adam，设置参数betas=(0.9, 0.99)\n",
    "opt_Adam = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59d3fdd",
   "metadata": {},
   "source": [
    "## 5.4.2 Dataset和DataLoader\n",
    "\n",
    "（1）创建和使用dataset\n",
    "\n",
    "在使用torch.utils.data.datset抽象类之前，首先要导入该类：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b4eec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data.dataset as Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ce1d0e",
   "metadata": {},
   "source": [
    "在应用Dataset抽象类创建子类时，通常需要重写__ inti __ 初始化方法来定义数据内容和标签，重写 __ len __ 方法来返回数据集大小，以及重写 __ getitem __ 方法来得到数据内容和标签。\n",
    "\n",
    "下面是一个简单的创建子类的示例。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb11228f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集大小为： 4\n",
      "(tensor([3., 2.]), tensor([0.]))\n",
      "(tensor([3., 2.]), tensor([0.]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data.dataset as Dataset\n",
    "import numpy as np\n",
    "\n",
    "#创建子类\n",
    "class subDataset(Dataset.Dataset):\n",
    "    #初始化，定义数据内容和标签\n",
    "    def __init__(self, Data, Label):\n",
    "        self.Data = Data\n",
    "        self.Label = Label\n",
    "    #返回数据集大小\n",
    "    def __len__(self):\n",
    "        return len(self.Data)\n",
    "    #得到数据内容和标签\n",
    "    def __getitem__(self, index):\n",
    "        data = torch.Tensor(self.Data[index])\n",
    "        label = torch.Tensor(self.Label[index])\n",
    "        return data, label\n",
    "\n",
    "# 构建数据集\n",
    "Data = np.asarray([[3, 2], [1, 4], [7, 2], [3, 1]])\n",
    "Label = np.asarray([[0], [0], [1], [1]])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sub = subDataset(Data, Label)  # 创建数据集对象\n",
    "    print('数据集大小为：', sub.__len__())  # 获取dataset数据的大小\n",
    "    print(sub.__getitem__(0))  # 第0项的数据\n",
    "    print(sub[0])  # 效果等同于__getitem__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99470866",
   "metadata": {},
   "source": [
    "以上示例中，用户通过自己创建的Dataset对象的子类来构建数据集对象，之后就可以使用该对象来读取数据集的相关数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428ae503",
   "metadata": {},
   "source": [
    "（2）创建和使用DataLoader\n",
    "\n",
    "DataLoader类提供了通过用户构建的数据集对象读取数据的方法。在使用该类时，首先进行导入：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8542942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data.dataloader as DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078cb506",
   "metadata": {},
   "source": [
    "在创建Dataloader迭代器对象时，需将将用户构建的数据集对象作为参数。例如创建要给DataLoader对象，该对象对应的数据集对象为sub，读取数据时，设置batchsize为2，表示每批处理两个数据，shuffle为false表示不打乱数据的顺序，num_workers=1表示使用1个子进程来处理加载数据，定义对象代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ecf9ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader.DataLoader(sub, batch_size=2,shuffle=False,num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7aa45c",
   "metadata": {},
   "source": [
    "之后就可以使用如下代码进行批量读取数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8ccf6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: tensor([[3., 2.],\n",
      "        [1., 4.]])\n",
      "label: tensor([[0.],\n",
      "        [0.]])\n",
      "data: tensor([[7., 2.],\n",
      "        [3., 1.]])\n",
      "label: tensor([[1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(dataloader):\n",
    "    data, label = item\n",
    "    print('data:', data)\n",
    "    print('label:', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9698ef",
   "metadata": {},
   "source": [
    "通过执行结果可以看到，每次读取到的数据是多个样本及其标签，样本数量是通过batch_size设定的。在实际训练或推理过程中，可以根据计算环境（如CPU、GPU）及每个样本的大小来选择合适的batch_size，使网络尽量可以以较大吞吐量并行快速计算，从而加快训练或推理的速度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1541183a",
   "metadata": {},
   "source": [
    "## 5.4.3 torchvision\n",
    "\n",
    "torchvision包是PyTorch的一个扩展包，收录了若干重要的公开数据集，网络模型和常用的图像变换方法，以便于研究者进行图像处理和识别方法的实验和学习。\n",
    "\n",
    "（1）torchvision.datasets数据集下载模块\n",
    "\n",
    "使用torchvision.datasets数据集下载模块提供的工具可以下载许多公开的经典数据集用于实验，如mnist手写数据集、CIFAR10图像十分类数据集等，下面演示如何使用torchvision.datasets模块下载CIFAR10数据集。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c7cc5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fd207f94ac441b6981dffd544369b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data.dataset as Dataset\n",
    "import torchvision\n",
    "# 全局取消证书验证,数据集更容易被下载成功\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# 分别下载训练集和测试集\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=None)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=None)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d79c60",
   "metadata": {},
   "source": [
    "（2）torchvision.models预训练模型模块\n",
    "\n",
    "torchvision.models模块封装了常用的各种神经网络结构，如alexnet、densenet、inception、resnet、VGG等，并提供了相应的预训练模型，通过简单调用便可以来读取网络结构和预训练模型，以便针对实际任务进行模型微调。\n",
    "\n",
    "以下代码将resnet50预训练模型加载：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbae2ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "model = torchvision.models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc15bf96",
   "metadata": {},
   "source": [
    "在导入resnet50预训练模型时，设置pretrained=True表示使用预训练模型的参数进行初始化网络参数，否则不使用预训练模型的参数进行初始化。\n",
    "\n",
    "以下代码将VGG16预训练模型加载，并打印其网格模型结构：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f86c3a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to C:\\Users\\86188/.cache\\torch\\hub\\checkpoints\\vgg16-397923af.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cdc9791409f4d4596a0520aa855566d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/528M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "model = models.vgg16(pretrained=True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8fc5a9",
   "metadata": {},
   "source": [
    "从模型的打印结果可以发现，最后的输出层有1000个神经元结点。在实际任务中，往往需要修改最后的输出层结点数，以便于使用新的训练样本针对新的任务进行训练。以下示例给出了修改上面的网络模型输出层为10个结点的示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70a9066d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Dropout(p=0.5, inplace=False)\n",
      "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (4): ReLU(inplace=True)\n",
      "  (5): Dropout(p=0.5, inplace=False)\n",
      "  (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.classifier[6]=torch.nn.Linear(4096,10)\n",
    "print(model.classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8622fb59",
   "metadata": {},
   "source": [
    "通常，我们只需要对最后几层网络的参数进行学习，而将前面各层的预训练好的参数固定下来。下面的示例给出了上述模型除最后三层的参数外其他所有参数固定下来的代码，即将要固定的参数的requires_grad的值为False。在训练时，优化器只需要对requires_grad的值为True的参数继续学习，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d90491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim as optimizer \n",
    "for p in model.parameters():\n",
    "    p.requires_grad=False\n",
    "for p in model.classifier.parameters():\n",
    "    p.requires_grad=True\n",
    "opt=optimizer.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b667c3a8",
   "metadata": {},
   "source": [
    "当然，也可以直接在定义优化器时直接给出需要学习的参数。因此，上述的代码也可以直接写成："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d280dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=optimizer.SGD(filter(lambda p: p.requires_grad, model.classifier.parameters()), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af891e70",
   "metadata": {},
   "source": [
    "接下来，就可以使用新样本对模型进行训练了。在训练时，就只对后三层网络的参数进行学习，其他各层的参数都是预训练好的，主要用于特征提取。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0eec4f",
   "metadata": {},
   "source": [
    "（3）torchvision.transforms图像变换模块\n",
    "\n",
    "torchvision.transforms模块提供的图像变换可以完成图像尺寸放缩、切割、翻转、边填充、归一化等操作，这些操作可以从原始图像中得到更多的图像，从而实现了图像增强，丰富了训练样本。\n",
    "\n",
    "以下代码给出了图像的各种变换示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4eff7c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 64, 64]) torch.Size([3, 64, 64])\n",
      "torch.Size([4, 3, 8, 8]) torch.Size([3, 8, 8])\n",
      "torch.Size([4, 3, 16, 16]) torch.Size([3, 16, 16])\n",
      "torch.Size([4, 3, 6, 6]) torch.Size([3, 6, 6])\n",
      "torch.Size([4, 3, 2, 2]) torch.Size([3, 2, 2])\n",
      "torch.Size([4, 3, 4, 4]) torch.Size([3, 4, 4])\n",
      "tensor([[[-0.1392, -0.4851],\n",
      "         [ 0.6462,  0.5202]],\n",
      "\n",
      "        [[ 0.9032,  0.0776],\n",
      "         [ 0.4410,  0.1151]],\n",
      "\n",
      "        [[ 1.0383,  0.1460],\n",
      "         [ 0.0371,  0.1873]]]), tensor([[[ 0.8608,  0.5149],\n",
      "         [ 1.6462,  1.5202]],\n",
      "\n",
      "        [[-0.0484, -0.4612],\n",
      "         [-0.2795, -0.4425]],\n",
      "\n",
      "        [[ 0.3461,  0.0487],\n",
      "         [ 0.0124,  0.0624]]])\n",
      "torch.Size([3, 2, 2]) (2, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\torchvision\\transforms\\transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import PIL\n",
    "orig_img1 = torch.randn(4, 3, 64, 64)\n",
    "orig_img2 = torch.randn(3, 64, 64)\n",
    "print(orig_img1.shape, orig_img2.shape)\n",
    "# 对图像进行放缩，默认插值方法是线性插值\n",
    "resize = torchvision.transforms.Resize((8, 8), interpolation=PIL.Image.BILINEAR)\n",
    "new_img1 = resize(orig_img1)\n",
    "new_img2 = resize(orig_img2)\n",
    "print(new_img1.shape, new_img2.shape)\n",
    "# 对图像进行中心切割\n",
    "ccrop = torchvision.transforms.CenterCrop((16,16))  # 切割得到的尺寸为(16,16)\n",
    "new_img3 = ccrop(orig_img1)\n",
    "new_img4 = ccrop(orig_img2)\n",
    "print(new_img3.shape, new_img4.shape)\n",
    "# 对图像进行随机切割\n",
    "rcrop = torchvision.transforms.RandomCrop(6)  # 切割得到的尺寸为(6,6)，注意参数为整数\n",
    "new_img5 = rcrop(orig_img1)\n",
    "new_img6 = rcrop(orig_img2)\n",
    "print(new_img5.shape, new_img6.shape)\n",
    "# 对图像先进行随机切割，然后再resize成给定的size大小\n",
    "rrcrop = torchvision.transforms.RandomResizedCrop(2)\n",
    "new_img7 = rrcrop(orig_img1)\n",
    "new_img8 = rrcrop(orig_img2)\n",
    "print(new_img7.shape, new_img8.shape)\n",
    "# 对图像边缘进行扩充\n",
    "pad = torchvision.transforms.Pad(padding=1, fill=0)  # 上下左右各扩充1行或1列值为0的像素\n",
    "new_img9 = pad(new_img7)\n",
    "new_img10 = pad(new_img8)\n",
    "print(new_img9.shape, new_img10.shape)\n",
    "# 对图像RGB各通道像素按正态分布进行归一化\n",
    "norm = torchvision.transforms.Normalize((-1,1,0), (1,2,3))\n",
    "new_img11 = norm(new_img7)\n",
    "new_img12 = norm(new_img8)\n",
    "print(f\"{new_img8}, {new_img12}\")\n",
    "# 转换为Image图像\n",
    "topil= torchvision.transforms.ToPILImage()\n",
    "new_img14 = topil(new_img8)\n",
    "print(new_img8.shape, new_img14.size)\n",
    "new_img14.save(\"a.png\")  # 将图像保存为文件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e66a61",
   "metadata": {},
   "source": [
    "当对图像依次进行多个变换操作时，可以使用torchvision.transforms.Compose类将这些变换连接在一起，构成一个统一的操作一次调用完成。示例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "884c2788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "orig_img =Image.open(\"a.png\")\n",
    "opts = transforms.Compose([\n",
    "     transforms.CenterCrop(10),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((-100,-150,-200), (10,5,3)),\n",
    "     transforms.ToPILImage()\n",
    "])\n",
    "new_img = opts(orig_img)\n",
    "new_img.save(\"b.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e3ae71",
   "metadata": {},
   "source": [
    "以上介绍了torchvision提供的几个常见的模块，对于其他模块（如utils等）在此不再介绍，读者可自行阅读相关资料或官方文档。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8798a74",
   "metadata": {},
   "source": [
    "## 5.4.4 torchaudio\n",
    "\n",
    "Torchaudio是PyTorch提供的一个音频处理和识别的包，内置了很多针对音频文件的I/O操作、音频变换及特征提取、音频数据集、音频处理及自动语音识别（ASR，Automatic Speech Recognition）预训练模型等等。在使用torchaudio之前，先确保已经安装，可使用如下命令进行安装："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f617b247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchaudio in d:\\anaconda\\lib\\site-packages (0.12.1)\n",
      "Requirement already satisfied: torch==1.12.1 in d:\\anaconda\\lib\\site-packages (from torchaudio) (1.12.1)\n",
      "Requirement already satisfied: typing-extensions in d:\\anaconda\\lib\\site-packages (from torch==1.12.1->torchaudio) (3.7.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a88d342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "print(torchaudio.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e901fa",
   "metadata": {},
   "source": [
    "下面的代码给出查看音频文件信息并进行显示的示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7d0d248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AudioMetaData(sample_rate=16000, num_frames=54400, num_channels=1, bits_per_sample=16, encoding=PCM_S)\n"
     ]
    }
   ],
   "source": [
    "import requests  #导入从网络下载文件用到的包\n",
    "import os\n",
    "SAMPLE_WAV_URL =\"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n",
    "SPEECH_FILE  = os.path.join(\".\", \"speech.wav\")\n",
    "if not os.path.exists(SPEECH_FILE):  # 如果文件在本地不存在，则下载该文件\n",
    "    with open(SPEECH_FILE, \"wb\") as file_:\n",
    "        file_.write(requests.get(SAMPLE_WAV_URL).content)\n",
    "        file_.close()\n",
    "metadata = torchaudio.info(SPEECH_FILE)  # 得到音频文件信息\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ddb4f5",
   "metadata": {},
   "source": [
    "通过结果可知，该音频文件采样率为16000Hz，共有54400个采样，单声道，采样采用16bit编码，使用PCM_S编码方式。我们可以在当前目录中找到该文件并使用播放器进行播放，内容为一段3.4秒的男生英文语音：“I had that curiosity beside me at this moment. ”。\n",
    "\n",
    "下面我们将使用预训练英文语音识别模型，对上例中的音频文件进行识别。代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a94c842b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I|HAD|THAT|CURIOSITY|BESIDE|ME|AT|THIS|MOMENT|\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 得到可用的计算资源\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
    "# 打开音频文件\n",
    "waveform, sample_rate = torchaudio.load(SPEECH_FILE)\n",
    "waveform = waveform.to(device)\n",
    "# 设置预训练模型\n",
    "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "model = bundle.get_model().to(device)\n",
    "# 确定合适的采用频率\n",
    "if sample_rate != bundle.sample_rate:\n",
    "    Waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)\n",
    "# 对音频数据进行分类，得到每个音素对应的每个类别的概率\n",
    "with torch.inference_mode():\n",
    "    emission, _ = model(waveform)\n",
    "# 定义针对分类结果的识别模型类    \n",
    "class GreedyCTCDecoder(torch.nn.Module):\n",
    "    def __init__(self, labels, blank=0):\n",
    "        super().__init__()\n",
    "        self.labels = labels\n",
    "        self.blank = blank\n",
    "\n",
    "    def forward(self, emission: torch.Tensor) -> str:      \n",
    "        indices = torch.argmax(emission, dim=-1)  # [num_seq,]\n",
    "        indices = torch.unique_consecutive(indices, dim=-1)\n",
    "        indices = [i for i in indices if i != self.blank]\n",
    "        return \"\".join([self.labels[i] for i in indices])\n",
    "# 识别模型类实例化\n",
    "decoder = GreedyCTCDecoder(labels=bundle.get_labels())\n",
    "# 针对分类结果进行识别，得到识别的字符串\n",
    "transcript = decoder(emission[0])\n",
    "# 打印最终的音频识别结果\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248d99f0",
   "metadata": {},
   "source": [
    "## 5.4.5 模型持久化方法\n",
    "\n",
    "与SKLearn的持久化类似，通过PyTorch训练好的模型往往需要部署到实际的业务系统中执行推理任务，这就需要对模型进行持久化，通常直接保存成文件即可。在实际的业务系统中使用时，再将模型文件加载到内存中使用。\n",
    "\n",
    "PyTorch有两种模型保存的方式，一种是保存整个网络结构信息和模型参数信息，另一种方式是只保存网络的模型参数，不保存网络结构。\n",
    "\n",
    "保存整个网络结构信息和模型参数信息的示例如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "caecd800",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(GreedyCTCDecoder, './model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6470d78e",
   "metadata": {},
   "source": [
    "该代码将训练好的模型model_object保存到当前目录下的model.pth文件。以下代码完成模型加载，加载后即可使用该模型进行推理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e1f6659",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c1c704",
   "metadata": {},
   "source": [
    "若仅保存网络的模型参数，则直接执行如下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4635bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_object.state_dict(), './params.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f64f51f",
   "metadata": {},
   "source": [
    "加载模型时，需要先导入网络，然后再加载参数，示例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1619f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import VggModel\n",
    "model = VggModel()\n",
    "model.load_state_dict(torch.load('./params.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbff31c",
   "metadata": {},
   "source": [
    "## 5.4.6 可视化工具包Visdom\n",
    "\n",
    "Facebook专为Pytorch开发的实时可视化工具包Visdom，常用于实时显示训练过程的数据，具有灵活高效、界面美观等特点。安装Visdom非常简单，例如可在终端输入如下命令："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8460136b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: visdom in d:\\anaconda\\lib\\site-packages (0.1.8.9)\n",
      "Requirement already satisfied: torchfile in d:\\anaconda\\lib\\site-packages (from visdom) (0.1.0)\n",
      "Requirement already satisfied: websocket-client in d:\\anaconda\\lib\\site-packages (from visdom) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.8 in d:\\anaconda\\lib\\site-packages (from visdom) (1.21.5)\n",
      "Requirement already satisfied: pyzmq in d:\\anaconda\\lib\\site-packages (from visdom) (20.0.0)\n",
      "Requirement already satisfied: scipy in d:\\anaconda\\lib\\site-packages (from visdom) (1.6.2)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (from visdom) (2.25.1)\n",
      "Requirement already satisfied: pillow in d:\\anaconda\\lib\\site-packages (from visdom) (8.2.0)\n",
      "Requirement already satisfied: tornado in d:\\anaconda\\lib\\site-packages (from visdom) (6.1)\n",
      "Requirement already satisfied: jsonpatch in d:\\anaconda\\lib\\site-packages (from visdom) (1.32)\n",
      "Requirement already satisfied: six in d:\\anaconda\\lib\\site-packages (from visdom) (1.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\anaconda\\lib\\site-packages (from jsonpatch->visdom) (2.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests->visdom) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests->visdom) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests->visdom) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in d:\\anaconda\\lib\\site-packages (from requests->visdom) (4.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install visdom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa3e011",
   "metadata": {},
   "source": [
    "当我们在编写训练程序时，可以在代码中加入在Visdom进行数据展示的代码。这样，在执行训练程序时，便可在浏览器登陆后的界面中，查看可视化的效果，\n",
    "\n",
    "下面代码演示Visdom根据数据点绘制曲线。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb5b8292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'train loss'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from visdom import Visdom\n",
    "viz = Visdom()  # 初始化visdom类，默认可视化环境为main\n",
    "viz.line([0.],  # Y的第一个点坐标\n",
    "        [0.],    # X的第一个点坐标\n",
    "        win=\"train loss\",    #窗口名称\n",
    "        opts=dict(title='train_loss')  # 图像标例\n",
    "        )  #设置起始点\n",
    "viz.line([1.],  # Y的下一个点坐标\n",
    "        [1.],    # X的下一个点坐标\n",
    "        win=\"train loss\",  # 窗口名称，与上个窗口同名表示显示在同一个表格里\n",
    "        update='append'    # 添加到上一个点后面\n",
    "        ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de6af61",
   "metadata": {},
   "source": [
    "上述示例是在默认的main环境下展示，Visdom也支持在多环境下显示不同的可视化结果，如下代码展示了在设置的image环境下展示图像："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32a8f6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'x'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from visdom import Visdom\n",
    "import numpy as np\n",
    "image = np.random.randn(1, 3, 200, 200) # 一张3通道,200*200大小的图像\n",
    "viz = Visdom(env='image') # 切换到image环境\n",
    "viz.images(image, win='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2472e2",
   "metadata": {},
   "source": [
    "下面的程序演示了模拟loss损失下降的过程，并对其进行可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fe14bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "from visdom import Visdom\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "x = np.linspace(1, 100, 1000)  # 在区间[1,100]中等间隔取1000个样本\n",
    "y = 1 / x  # 模拟损失的值\n",
    "y = y.reshape(1000, 1)  # y变为shape为(1000,1)的二维数组\n",
    "x = x.reshape(1000, 1)  # x变为shape为(1000,1)的二维数组\n",
    "\n",
    "vis = Visdom(env='loss')  # 创建一个loss窗口\n",
    "loss_window = vis.line(\n",
    "    X=x[0],\n",
    "    Y=y[0],\n",
    "    opts={'xlabel': 'epochs', 'ylabel': 'value', 'title': 'loss'}  # 先声明窗口的标题和坐标轴的名称\n",
    ")\n",
    "\n",
    "for i in range(1, 1000):\n",
    "    time.sleep(0.2) #模拟loss动态更新的效果\n",
    "    vis.line(\n",
    "        X=x[i],\n",
    "        Y=y[i],\n",
    "        win=loss_window,\n",
    "        update='append'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
